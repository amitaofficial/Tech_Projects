{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CTFIDF.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOyZTK11GiLhJOGTTfS6l8F"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26QOVoB9Wknw",
        "outputId": "25278b16-c874-42e9-e333-a4a3a51d1439"
      },
      "source": [
        "!pip install pyspark\n",
        "import sys\n",
        " \n",
        "from pyspark import SparkContext, SparkConf\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/b0/9d6860891ab14a39d4bddf80ba26ce51c2f9dc4805e5c6978ac0472c120a/pyspark-3.1.1.tar.gz (212.3MB)\n",
            "\u001b[K     |████████████████████████████████| 212.3MB 68kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 20.8MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.1.1-py2.py3-none-any.whl size=212767604 sha256=6007684b5bbf2c59b7986be25c514258ce1c7583602deacdc4eea1cd30d32c29\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/90/c0/01de724414ef122bd05f056541fb6a0ecf47c7ca655f8b3c0f\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9 pyspark-3.1.1\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDMd1yfjMnZo"
      },
      "source": [
        "import math\n",
        "import re\n",
        "################################# functions#######################################\n",
        "####### remove stopwords #####\n",
        "def removeStopWords(line):\n",
        "  words = [word for word in line.split(\" \") if word.lower() not in stopwordsSet]\n",
        "  new_text = \" \".join(words)\n",
        "  return new_text\n",
        "\n",
        "####### remove URLS #####\n",
        "def removeUrl(line):\n",
        "  return re.sub(r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’]))\", \"\", line)\n",
        "\n",
        "####### remove Punctuation #####\n",
        "def removePunctuation(line):\n",
        "  words = line.split(\" \")\n",
        "  removedWords = []\n",
        "  for word in words:\n",
        "    word2 = re.sub(r'[^\\w\\s\\n<docid></docid><text></text>]', '', word)\n",
        "    if word2 != '':\n",
        "            removedWords.append(word2)\n",
        "  new_text = \" \".join(removedWords)\n",
        "  return new_text\n",
        "\n",
        "####### remove Tags #####\n",
        "def removeTags(line):\n",
        "  return re.compile(r'<(?!docid|text|/docid|/text).*?>').sub('',line)\n",
        "\n",
        "#get words and their docids \n",
        "def wordDocId(input):\n",
        "  str = input.split(\"<docid>\")\n",
        "  data = []\n",
        "  for a in str:\n",
        "    if a:\n",
        "      documentno = a.split(\"</docid>\")[0]\n",
        "      # data.append(documentno)\n",
        "      texthere = a.split(\"</docid>\")[1].replace('<text>','').replace('</text>','')\n",
        "      # data.append(texthere)\n",
        "      words = texthere.split(\" \")\n",
        "      for word in words:\n",
        "        str = word,documentno\n",
        "        data.append(str)\n",
        "  return data\n",
        "\n",
        "############ create term frequency matrix ##################### \n",
        "def createTermFreqMatrix(line):\n",
        "  ((word,docId),freq) = line\n",
        "  freq = 1+ math.log10(freq) # log weighted frequency\n",
        "  return (word,(docId,freq))\n",
        "\n",
        "#create document frequency\n",
        "def calculateDocFrequency(line):\n",
        "  (word,dataArray) = line\n",
        "  inverseFreq = math.log10(numberOfDocuments/1+len(dataArray))\n",
        "  return (word,inverseFreq)\n",
        "\n",
        "######################## TF-IDF ##############################\n",
        "def tfidf(line):\n",
        "  (word,(dataArray,docFreq)) = line  ##ex:[('siege', ([('10000', 1.6020599913279625)], 0.6989700043360189)),...\n",
        "  newDataArray = []\n",
        "  for data in dataArray:\n",
        "    (docid,termFreq) = data\n",
        "    tfIdfFreq = termFreq*docFreq #TF-IDF \n",
        "    newData = (docid,tfIdfFreq)\n",
        "    newDataArray.append(newData)\n",
        "  return (word,newDataArray)\n",
        "\n",
        "####### refactor RDD ######\n",
        "def refactorRDD(array):\n",
        "  return array\n",
        "\n",
        "####### euclidean length of each document ######\n",
        "def euclideanLength(line):\n",
        "  (docid,array) = line\n",
        "  sum = 0\n",
        "  for data in array:\n",
        "    sum += (data*data) \n",
        "  return (docid,math.sqrt(sum))\n",
        "\n",
        "#write in the format of word@doc-1#freq+doc-2#freq+doc-3#freq+...\n",
        "def customWriteToFile(line):\n",
        " (word,matrixArray) = line\n",
        " data = []\n",
        " var = word + \"@\"\n",
        " for terms in matrixArray:\n",
        "   (docId,freq) = terms\n",
        "   var += str(docId)+\"#\"+str(freq)+\"+\"\n",
        " return var\n",
        "\n",
        "############## make the tf cosine normalized ######################\n",
        "def cosineNorm(line):   # [('siege', [('10000', 1.1197918790850683)]),...\n",
        "  (word,dataArray) = line\n",
        "  newDataArray = []\n",
        "  for data in dataArray:\n",
        "    (docid,freq) = data\n",
        "    freq = freq/docEuclidLengthDictionary[docid]\n",
        "    newTuple = (docid,freq)\n",
        "    newDataArray.append(newTuple)\n",
        "  return (word,newDataArray)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKEoSfFvWtGL"
      },
      "source": [
        "conf = SparkConf()\n",
        "conf.setMaster('local')\n",
        "conf.setAppName('InfoRetrieval')\n",
        "sc = SparkContext.getOrCreate(conf=conf)\n",
        "file1 = '2.txt' #Enter file name/path here\n",
        "fileRDD = sc.textFile(file1)\n",
        "numberOfDocuments = fileRDD.count() #get no: of documents\n",
        "\n",
        "lowercaseRDD = fileRDD.map(lambda line:line.lower()) # case folding\n",
        "\n",
        "#remove stopwords\n",
        "stopwordsSet = set(line.strip() for line in open('stopwords.txt'))\n",
        "stopWordsRemovedRDD = lowercaseRDD.map(removeStopWords)\n",
        "# print(\"check is:\",stopWordsRemovedRDD.collect())\n",
        "\n",
        "#remove url\n",
        "urlRemovedRDD = stopWordsRemovedRDD.map(removeUrl)\n",
        "\n",
        "#remove Punctuation\n",
        "punctRemovedRDD = urlRemovedRDD.map(removePunctuation)\n",
        "\n",
        "#remove tags\n",
        "tagsRemovedRDD = punctRemovedRDD.map(removeTags)\n",
        "\n",
        "\n",
        "#prep RDD to calculate term frequencies\n",
        "testRDD = tagsRemovedRDD.flatMap(wordDocId) #ex: [('siege', '10000'), ('jerusalem', '10000')....\n",
        "# print(testRDD.collect())\n",
        "#calculate same tuples(or words) and combine them together\n",
        "testRdd2 = testRDD.map(lambda word:(word,1))#ex: [(('siege', '10000'), 1),...\n",
        "testRdd3 = testRdd2.reduceByKey(lambda a,b: a+b)#ex: [(('siege', '10000'), 4)...\n",
        "\n",
        "############ create term frequency matrix ##################### \n",
        "rdd4 = testRdd3.map(createTermFreqMatrix) #ex: [('siege', ('10000', 1.6020599913279625)),...\n",
        "\n",
        "#combine all same words together \n",
        "freqMatrix = rdd4.groupByKey().mapValues(list)#ex: [('siege', [('10000', 1.6020599913279625)]),...\n",
        "# print(freqMatrix.collect())\n",
        "##################################################################\n",
        "\n",
        "# ######################## Document Frequency ##############################\n",
        "\n",
        "documentFreq =  freqMatrix.map(calculateDocFrequency) #will have only document frequency of a word\n",
        "# print(documentFreq.collect()) #ex:[('siege', 0.6989700043360189),...\n",
        "\n",
        "\n",
        "######################## TF-IDF ##############################\n",
        "\n",
        "tfidfJoinRdd = freqMatrix.join(documentFreq) #join term frequency matrix to document frequency\n",
        "# print(tfidfJoinRdd.collect())\n",
        "\n",
        "tfidfRdd = tfidfJoinRdd.map(tfidf) #calculate TFIDF on joined dataset ex:[('siege', [('10000', 1.1197918790850683)])...\n",
        "# print(tfidfRdd.collect())\n",
        "\n",
        "#################### for TFIDF###############################\n",
        "tempRdd = tfidfRdd.values() #get only docid and tfid values\n",
        "# print(tempRdd.collect()) #[('10000', 1.1197918790850683)], [('10000', 0.6989700043360189)].....\n",
        "#################### for TFIDF###############################\n",
        "\n",
        "rdd3 = tempRdd.flatMap(refactorRDD).groupByKey().mapValues(list) #refactor to get an RDD of (docid,list of termfrequencies in that docid)\n",
        "# print(rdd3.collect()) #[('10000', [1.1197918790850683, 0.69897000433601...\n",
        "\n",
        "docEuclidLengthsRdd = rdd3.map(euclideanLength) # get euclidean lengths of each document\n",
        "# print(docEuclidLengthsRdd.collect()) #[('10000', 4.998304130287607), ('10001', 10.697526447828157)..\n",
        "\n",
        "############## make the tfidf cosine normalized ######################\n",
        "\n",
        "docEuclidLengthDictionary = docEuclidLengthsRdd.collectAsMap() #maintain a dictionary for document frequencies for easy reference\n",
        "# print(docEuclidLengthDictionary)\n",
        "\n",
        "cosineNormalizedRdd = freqMatrix.map(cosineNorm) #cosine normalize weighted term frequencies\n",
        "# print(cosineNormalizedRdd.collect())\n",
        "\n",
        "#  write to file - create CTFIDF_index\n",
        "cosineNormalizedRdd2 = cosineNormalizedRdd.map(customWriteToFile)\n",
        "# print(cosineNormalizedRdd2.collect())\n",
        "cosineNormalizedRdd2.saveAsTextFile('/content/drive/MyDrive/test/CTFIDF_Index')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}